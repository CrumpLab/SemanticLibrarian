---
title: "Semantic Librarian Blog"
author: "Matt Crump"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
twitter: "https://twitter.com/MattCrump_"
github: "https://github.com/CrumpLab"
website: "https://crumplab.gihub.io"
bibliography: refs.bib
csl: apa.csl
output: 
  html_document:
    template: web/template.html
    toc: true
    toc_float: true
    collapsed: false
    code_folding: hide
    number_sections: false
    toc_depth: 4
    theme: yeti
    highlight: kate
    css: web/crump_basic.css
    includes:
      in_header: web/header.html
    md_extensions: -autolink_bare_uris
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

---

The Semantic Librarian is an R Shiny App developed by [Matthew Crump](https://crumplab.github.io) (Brooklyn College of the City University of New York), [Randall Jamieson](https://umcognitivesciencelaboratory.weebly.com), Matt Cook (University of Manitoba), and [Harinder Aujla](http://ion.uwinnipeg.ca/~haujla/) (University of Winnipeg). We use a vector-space model of word-semantics to all a user to search a set of documents by their semantic similarity. A current version of the app can be found here [https://crumplab.shinyapps.io/SemanticLibrarian](https://crumplab.shinyapps.io/SemanticLibrarian). This version used select experimental journals from the APA (American Psychological Association) corpus, published between the 1890s and 2016.

This blog will record some further examination and analyses of the APA database. 

## The Database

The list of journals is:

```{r, results='asis'}
load("../allData/allData.RData", envir=.GlobalEnv)
the_journals<-levels(article_df$journal)
for(i in 1:length(the_journals)){
 cat("- ",the_journals[i], sep="")
 cat("\n")
}
```

There were a total of `r dim(article_df)[1]` abstracts, `r length(author_list)` authors, and the corpus consisted of `r length(dictionary_words)` words. We used BEAGLE [@jones2007representing] to create semantic vectors for each word. Then we created abstract vectors for each article by adding the word vectors together (that were in each article). We also created vectors for each author, as the sum of their abstract vectors. This way, all of the words, abstracts, and authors, can be projected into the same high-dimensional semantic space. As a result, it is possible to compute the semantic similarity between any word, abstract, or author. This blog explores some of these relationships.

## Top 10 abstracts

5/29/2019

What are the top 10 abstracts published in these APA journals? Who knows. What would the measure be?

How about a different question, which 10 abstracts are the most similar to all other abstracts? Let's find out. Below is a table of the top 10 abstracts that have the highest mean similarity to all of the other abstracts. To find this, I computed the cosine similarity between each abstract vector and every other abstract vector. That results in a 27560 x 27560 similarity matrix. Each column represents the similarities of an article to all the other articles. I computed the mean for each column, then below, I list the top 10 with the largest means.

```{r}
load("article_df.RData")
library(dplyr)
top_10_articles <- article_df %>%
                    arrange(desc(mean_sim)) %>%
                    slice(1:10) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_10_articles)
```

Interesting. I've read a couple of these papers over the years. Estes (1976) is a classic. Whittlesea, Brooks, and Westcott (1994) is a fantastic paper, kind of surprised to see it pop up here. 

In many ways I'm not sure what to make of this information. The similarities are all around .93. The abstracts span many years of publication, and multiple journals are represented. Did all of these articles have the same kind of impact on the field? The database currently does not have citation information, and that would be worth adding at some point. At the very least, it wasn't clear to me before doing this whether the top 10 most similar abstracts would be "good" papers, representing something centrally important about these domains, or whether they would be derivative papers, that are like many other papers, but perhaps not as noteworthy. 

### Most similar papers by year

How about one more table. Let's look at the most similar abstracts for each year. Within in each year, I found the paper that had the highest mean similarity to all other papers (across all years). The table below shows the top papers (highest similarity to the set) across the last 50 years.

```{r}

top_10_articles_year <- article_df %>%
                        arrange(desc(year)) %>%
                        group_by(year) %>%
                        filter(mean_sim == max(mean_sim)) %>%
                        ungroup() %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sim)

knitr::kable(top_10_articles_year)
```

Again, not really sure what to make of this list. Some of these papers are familiar to me, some are not. At the very least, this is a data-driven measure of something about these abstracts, but there were also many assumptions built into the construction of the semantic vectors, and it remains unclear how those assumptions would change the outcome here. Nevertheless, it is interesting to speculate that this kind of approach yields information about the quality of articles, in a similar way to the citation analysis used by @cho2012citation, to create a list of top papers in cognitive psychology. 

## Semantically distinct abstracts

5/30/2019

What are the most semantically distinct abstracts? These might the abstracts with the lowest mean similarity to all other abstracts. In that sense, these abstracts are the ones that are furthest away from the others in semantic space; so they are distinct in terms of their differentness to the entire set. Let's look at the top 25 and see what we get.

```{r}
top_25_distinct <- article_df %>%
                    arrange(mean_sim) %>%
                    slice(1:25) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_25_distinct)
```

Ok, not really what I was looking for, but sensible. The database contains many entries that are not normal abstracts for empirical papers. They could be editorials by incoming editors, obituaries, and apparently, abstracts not written in English. So, it makes sense that the abstracts above are the kind of abstracts that are most dissimilar to the entire set, which is mostly composed of abstracts for empirical papers.

To look at the abstracts for regular articles, I suppose the database could be tagged, but there are too many entries for me to do that right now. Let's take a look at the distribution of mean similarity values first, and then perhaps decide to exclude the really rare and weird abstracts based on some exclusion criterion.

```{r}
library(ggplot2)
ggplot(article_df, aes(x=mean_sim)) +
  geom_histogram(bins=100, size=1, color="white") +
  xlab("Mean Similarity")+
  ggtitle("Histogram of mean similarity between each article and every other article")
```

It's too small to see, but there are some abstracts that span the range into the lower values. Let's take a look at the 25 articles with the lowest mean similarity, starting with a cutoff of .7.

```{r}
top_25_distinct <- article_df %>%
                    arrange(mean_sim) %>%
                    filter(mean_sim > .7) %>%
                    slice(1:25) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_25_distinct[1:5,])
```


I'll spare the reader, and only show the top 5--still abstracts not related to regular papers. Let's start at .8 instead.

```{r}
top_25_distinct <- article_df %>%
                    arrange(mean_sim) %>%
                    filter(mean_sim > .8) %>%
                    slice(1:25) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_25_distinct)
```

Ok, now we are starting to look at more regular articles. And, what do we see? It seems like mainly older articles. Perhaps not surprising. There are fewer of those in the set, and their abstract lengths are typically shorter than newer articles. 

## Central articles and different metrics

I previously used mean similarity to look at "central" articles. Let's use a few more descriptive statistics to get at similar issues. There's a whole bunch of things to try here. I'm going to start with two things. For example, what if we divide mean similarity, by the standard deviation? There's some good reasons to do that. But before that, let's look first at the standard deviation.

### Articles with highest standard deviation

Using the cosine similarity matrix, I computed the standard deviation of similiarity scores in each column. This gives a measure of dispersion for each article. What do the top 25 articles with the lowest standard deviation look like? Whatever these articles are, they are surrounded the most tightly by the other articles. 

```{r}
top_25_sd <- article_df %>%
                    arrange(sd_sim) %>%
                    slice(1:25) %>%
                    mutate(sd_sim = round(sd_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, sd_sim)
knitr::kable(top_25_sd)

```

Well, I don't think I'll be going and reading these articles. The top 25, with the lowest standard deviation appear to be mostly older papers.

Now, what I really wanted to do was to divide the mean similarities, by the standard deviations; then looks at the papers with the highest values. Papers with the highest values will be the ones with larger mean similarity, and small standard deviation. These are the kinds of papers that are perhaps the most central inside the set. They are the closest to all other papers, and they surrounded more closely by the other papers.

```{r}
top_50_mean_sd <- article_df %>%
                    mutate(mean_sd = mean_sim / sd_sim) %>%
                    arrange(desc(mean_sd)) %>%
                    slice(1:50) %>%
                    mutate(mean_sd = round(mean_sd,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sd)
knitr::kable(top_50_mean_sd)

```


Huh. Wasn't expecting this kind of list. Lots of older papers, and a very different list from the earlier section looking at highest mean similarity. Still, curious to look at this same thing by year. So, the next table find the articles withe highest value (mean similarity divided by standard deviation) in each year. I'll show the last 50 years:

```{r}

top_50_articles_year <- article_df %>%
                        mutate(mean_sd = mean_sim / sd_sim) %>%
                        arrange(desc(year)) %>%
                        group_by(year) %>%
                        filter(mean_sd == max(mean_sd)) %>%
                        ungroup() %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sd)

knitr::kable(top_50_articles_year)
```

## Looking at the journals

### Psychological Review

I'm going to take a first stab at looking into specific journals. Let's start with Psychological Review, a well-respected journal. First, let's look at the top 50 papers in Psych review with the highest mean similarities to all other abstracts across all journals:

```{r}
psych_review_top_50 <- article_df %>%
                        filter(journal == "Psychological Review") %>%
                        arrange(desc(mean_sim)) %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sim)

knitr::kable(psych_review_top_50)
```

Some good papers in there, but hey, it's Psych Review. Another aspect is to look entirely within Psych review papers. So, rather than finding which papers in Psych Review are most similar to all other papers, let's just find the papers that are most similar to the other papers in Psych Review.

```{r}
load("psyc_review_df.RData")
psych_review_top_50_w <- psyc_review_df %>%
                        arrange(desc(pr_sims)) %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sim)
knitr::kable(psych_review_top_50_w)
```

Well, it's another different list, and more tea-leaf reading. Without other metrics to validate these measures against it's not clear what is being measured.

---

## References



