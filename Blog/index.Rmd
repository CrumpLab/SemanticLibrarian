---
title: "Semantic Librarian Blog"
author: "Matt Crump"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
twitter: "https://twitter.com/MattCrump_"
github: "https://github.com/CrumpLab"
website: "https://crumplab.gihub.io"
bibliography: refs.bib
csl: apa.csl
output: 
  html_document:
    template: web/template.html
    toc: true
    toc_float: true
    collapsed: false
    code_folding: hide
    number_sections: false
    toc_depth: 4
    theme: yeti
    highlight: kate
    css: web/crump_basic.css
    includes:
      in_header: [web/header.html,web/ga_script.js]
    md_extensions: -autolink_bare_uris
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

---

The Semantic Librarian is an R Shiny App developed by [Matthew Crump](https://crumplab.github.io) (Brooklyn College of the City University of New York), [Randall Jamieson](https://umcognitivesciencelaboratory.weebly.com), Matt Cook (University of Manitoba), and [Harinder Aujla](http://ion.uwinnipeg.ca/~haujla/) (University of Winnipeg). We use a vector-space model of word-semantics to all a user to search a set of documents by their semantic similarity. A current version of the app can be found here [https://crumplab.shinyapps.io/SemanticLibrarian](https://crumplab.shinyapps.io/SemanticLibrarian). This version used select experimental journals from the APA (American Psychological Association) corpus, published between the 1890s and 2016.

This blog will record some further examination and analyses of the APA database. 

## The Database

The list of journals is:

```{r, results='asis'}
load("../allData/allData.RData", envir=.GlobalEnv)
the_journals<-levels(article_df$journal)
for(i in 1:length(the_journals)){
 cat("- ",the_journals[i], sep="")
 cat("\n")
}
```

There were a total of `r dim(article_df)[1]` abstracts, `r length(author_list)` authors, and the corpus consisted of `r length(dictionary_words)` words. We used BEAGLE [@jones2007representing] to create semantic vectors for each word. Then we created abstract vectors for each article by adding the word vectors together (that were in each article). We also created vectors for each author, as the sum of their abstract vectors. This way, all of the words, abstracts, and authors, can be projected into the same high-dimensional semantic space. As a result, it is possible to compute the semantic similarity between any word, abstract, or author. This blog explores some of these relationships.

## Top 10 abstracts

5/29/2019

What are the top 10 abstracts published in these APA journals? Who knows. What would the measure be?

How about a different question, which 10 abstracts are the most similar to all other abstracts? Let's find out. Below is a table of the top 10 abstracts that have the highest mean similarity to all of the other abstracts. To find this, I computed the cosine similarity between each abstract vector and every other abstract vector. That results in a 27560 x 27560 similarity matrix. Each column represents the similarities of an article to all the other articles. I computed the mean for each column, then below, I list the top 10 with the largest means.

```{r}
load("article_df.RData")
library(dplyr)
top_10_articles <- article_df %>%
                    arrange(desc(mean_sim)) %>%
                    slice(1:10) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_10_articles)
```

Interesting. I've read a couple of these papers over the years. Estes (1976) is a classic. Whittlesea, Brooks, and Westcott (1994) is a fantastic paper, kind of surprised to see it pop up here. 

In many ways I'm not sure what to make of this information. The similarities are all around .93. The abstracts span many years of publication, and multiple journals are represented. Did all of these articles have the same kind of impact on the field? The database currently does not have citation information, and that would be worth adding at some point. At the very least, it wasn't clear to me before doing this whether the top 10 most similar abstracts would be "good" papers, representing something centrally important about these domains, or whether they would be derivative papers, that are like many other papers, but perhaps not as noteworthy. 

### Most similar papers by year

How about one more table. Let's look at the most similar abstracts for each year. Within in each year, I found the paper that had the highest mean similarity to all other papers (across all years). The table below shows the top papers (highest similarity to the set) across the last 50 years.

```{r}

top_10_articles_year <- article_df %>%
                        arrange(desc(year)) %>%
                        group_by(year) %>%
                        filter(mean_sim == max(mean_sim)) %>%
                        ungroup() %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sim)

knitr::kable(top_10_articles_year)
```

Again, not really sure what to make of this list. Some of these papers are familiar to me, some are not. At the very least, this is a data-driven measure of something about these abstracts, but there were also many assumptions built into the construction of the semantic vectors, and it remains unclear how those assumptions would change the outcome here. Nevertheless, it is interesting to speculate that this kind of approach yields information about the quality of articles, in a similar way to the citation analysis used by @cho2012citation, to create a list of top papers in cognitive psychology. 

## Semantically distinct abstracts

5/30/2019

What are the most semantically distinct abstracts? These might the abstracts with the lowest mean similarity to all other abstracts. In that sense, these abstracts are the ones that are furthest away from the others in semantic space; so they are distinct in terms of their differentness to the entire set. Let's look at the top 25 and see what we get.

```{r}
top_25_distinct <- article_df %>%
                    arrange(mean_sim) %>%
                    slice(1:25) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_25_distinct)
```

Ok, not really what I was looking for, but sensible. The database contains many entries that are not normal abstracts for empirical papers. They could be editorials by incoming editors, obituaries, and apparently, abstracts not written in English. So, it makes sense that the abstracts above are the kind of abstracts that are most dissimilar to the entire set, which is mostly composed of abstracts for empirical papers.

To look at the abstracts for regular articles, I suppose the database could be tagged, but there are too many entries for me to do that right now. Let's take a look at the distribution of mean similarity values first, and then perhaps decide to exclude the really rare and weird abstracts based on some exclusion criterion.

```{r}
library(ggplot2)
ggplot(article_df, aes(x=mean_sim)) +
  geom_histogram(bins=100, size=1, color="white") +
  xlab("Mean Similarity")+
  ggtitle("Histogram of mean similarity between each article and every other article")
```

It's too small to see, but there are some abstracts that span the range into the lower values. Let's take a look at the 25 articles with the lowest mean similarity, starting with a cutoff of .7.

```{r}
top_25_distinct <- article_df %>%
                    arrange(mean_sim) %>%
                    filter(mean_sim > .7) %>%
                    slice(1:25) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_25_distinct[1:5,])
```


I'll spare the reader, and only show the top 5--still abstracts not related to regular papers. Let's start at .8 instead.

```{r}
top_25_distinct <- article_df %>%
                    arrange(mean_sim) %>%
                    filter(mean_sim > .8) %>%
                    slice(1:25) %>%
                    mutate(mean_sim = round(mean_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sim)
knitr::kable(top_25_distinct)
```

Ok, now we are starting to look at more regular articles. And, what do we see? It seems like mainly older articles. Perhaps not surprising. There are fewer of those in the set, and their abstract lengths are typically shorter than newer articles. 

## Central articles and different metrics

I previously used mean similarity to look at "central" articles. Let's use a few more descriptive statistics to get at similar issues. There's a whole bunch of things to try here. I'm going to start with two things. For example, what if we divide mean similarity, by the standard deviation? There's some good reasons to do that. But before that, let's look first at the standard deviation.

### Articles with highest standard deviation

Using the cosine similarity matrix, I computed the standard deviation of similiarity scores in each column. This gives a measure of dispersion for each article. What do the top 25 articles with the lowest standard deviation look like? Whatever these articles are, they are surrounded the most tightly by the other articles. 

```{r}
top_25_sd <- article_df %>%
                    arrange(sd_sim) %>%
                    slice(1:25) %>%
                    mutate(sd_sim = round(sd_sim,digits=3)) %>%
                    select(title,authorlist,journal,year, sd_sim)
knitr::kable(top_25_sd)

```

Well, I don't think I'll be going and reading these articles. The top 25, with the lowest standard deviation appear to be mostly older papers.

Now, what I really wanted to do was to divide the mean similarities, by the standard deviations; then looks at the papers with the highest values. Papers with the highest values will be the ones with larger mean similarity, and small standard deviation. These are the kinds of papers that are perhaps the most central inside the set. They are the closest to all other papers, and they surrounded more closely by the other papers.

```{r}
top_50_mean_sd <- article_df %>%
                    mutate(mean_sd = mean_sim / sd_sim) %>%
                    arrange(desc(mean_sd)) %>%
                    slice(1:50) %>%
                    mutate(mean_sd = round(mean_sd,digits=3)) %>%
                    select(title,authorlist,journal,year, mean_sd)
knitr::kable(top_50_mean_sd)

```


Huh. Wasn't expecting this kind of list. Lots of older papers, and a very different list from the earlier section looking at highest mean similarity. Still, curious to look at this same thing by year. So, the next table find the articles withe highest value (mean similarity divided by standard deviation) in each year. I'll show the last 50 years:

```{r}

top_50_articles_year <- article_df %>%
                        mutate(mean_sd = mean_sim / sd_sim) %>%
                        arrange(desc(year)) %>%
                        group_by(year) %>%
                        filter(mean_sd == max(mean_sd)) %>%
                        ungroup() %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sd)

knitr::kable(top_50_articles_year)
```

## Looking at the journals

### Psychological Review

I'm going to take a first stab at looking into specific journals. Let's start with Psychological Review, a well-respected journal. First, let's look at the top 50 papers in Psych review with the highest mean similarities to all other abstracts across all journals:

```{r}
psych_review_top_50 <- article_df %>%
                        filter(journal == "Psychological Review") %>%
                        arrange(desc(mean_sim)) %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sim)

knitr::kable(psych_review_top_50)
```

Some good papers in there, but hey, it's Psych Review. Another aspect is to look entirely within Psych review papers. So, rather than finding which papers in Psych Review are most similar to all other papers, let's just find the papers that are most similar to the other papers in Psych Review.

```{r}
load("psyc_review_df.RData")
psych_review_top_50_w <- psyc_review_df %>%
                        arrange(desc(pr_sims)) %>%
                        slice(1:50) %>%
                        select(title,authorlist,journal,year, mean_sim)
knitr::kable(psych_review_top_50_w)
```

Well, it's another different list, and more tea-leaf reading. Without other metrics to validate these measures against it's not clear what is being measured.

## Journals over Time

I'll come back here to say more about this when I have the tools to say more. The question is about how the semantic structure of the database changes over time. I have to think more about ways to answer this question.

For now, here is an animation of publications in the different journals over time. The color of the dots represents different journals. More to follow when I get the chance.

```{r}
knitr::include_graphics("imgs/APA_years.gif")
```


## Top 100 most similar authors

Let's a look at the most "central" authors in the database. To do this, I computed the similarity between each author and every other author. Every author has a semantic vecctor, which is the sum of their abstract vectors. I created the author similarity matrix, and the found the column means. Then I found the top 100 authors with the highest mean similarity. Below is a 2-d plot using multi-dimensional scaling, showing the top 100 authors.

```{r}
load("author_mean_sims.RData")

names(author_mean_sims) <- author_list
author_mean_sims <- sort(author_mean_sims, decreasing=T)
#author_mean_sims[1:100]

library(RsemanticLibrarian)

top_author_names <- names(author_mean_sims)[1:100]
author_ids <- which(author_list %in% top_author_names == TRUE)
restricted_author_vecs <- AuthorVectors[author_ids,]
row.names(restricted_author_vecs)<-NULL
library(lsa)
top_author_sims <- cosine(t(restricted_author_vecs))

fits <- cmdscale(1-top_author_sims,eig=TRUE, k=2)
cluster <- kmeans(fits$points,10)
fit_df <- data.frame(fits$points,authors = names(author_mean_sims[1:100]),
                     Similarity = author_mean_sims[1:100],
                     cluster = as.factor(cluster$cluster))
```

```{r, fig.width=10, fig.height=5, eval=FALSE}
library(ggrepel)
ggplot(fit_df, aes(x=X1, y=X2, color = cluster,
                                label=authors))+
        geom_hline(yintercept=0, color="grey")+
        geom_vline(xintercept=0, color="grey")+
        geom_point(aes(size=Similarity), alpha=.75)+
        geom_text_repel(aes(size=Similarity),color="black", force=1.5)+
        scale_size(range = c(4, 8))+
        theme_void()+
        theme(legend.position = "none")+
        ggtitle("APA authors (1890s-2016, Experimental Journals) with highest semantic similarity to other authors")+
        theme(plot.title = element_text(size = 30, face = "bold"))
```


```{r}
knitr::include_graphics("imgs/Top100APAauthors.png")
```

Well, there they are. Lots of recognizable authors. Daniel Kahneman had the highest mean similarity. He is not in the center, as you might expect. This is partly due to the multi-dimensional scaling solution. The mean similarities for each author are captured by the size of the dot. 

### top 200

And for fun, here are the next 100 (101- 200) authors with the highest mean similarity.

```{r, eval =FALSE}
load("author_mean_sims.RData")

names(author_mean_sims) <- author_list
author_mean_sims <- sort(author_mean_sims, decreasing=T)
#author_mean_sims[1:100]

library(RsemanticLibrarian)

top_author_names <- names(author_mean_sims)[101:200]
author_ids <- which(author_list %in% top_author_names == TRUE)
restricted_author_vecs <- AuthorVectors[author_ids,]
row.names(restricted_author_vecs)<-NULL
library(lsa)
top_author_sims <- cosine(t(restricted_author_vecs))

fits <- cmdscale(1-top_author_sims,eig=TRUE, k=2)
cluster <- kmeans(fits$points,10)
fit_df <- data.frame(fits$points,authors = names(author_mean_sims[101:200]),
                     Similarity = author_mean_sims[101:200],
                     cluster = as.factor(cluster$cluster))
```

```{r, fig.width=10, fig.height=5, eval=FALSE}
library(ggrepel)
ggplot(fit_df, aes(x=X1, y=X2, color = cluster,
                                label=authors))+
        geom_hline(yintercept=0, color="grey")+
        geom_vline(xintercept=0, color="grey")+
        geom_point(aes(size=Similarity), alpha=.75)+
        geom_text_repel(aes(size=Similarity),color="black", force=1.5)+
        scale_size(range = c(4, 8))+
        theme_void()+
        theme(legend.position = "none")+
        ggtitle("Top 101 - 200 APA authors (1890s-2016, Experimental Journals) \n with highest semantic similarity to other authors")+
        theme(plot.title = element_text(size = 30, face = "bold", hjust = .5))
```

```{r}
knitr::include_graphics("imgs/Top101200Authors.png")
```

### What does it mean?

06/1/2019

These graphs raise all sorts of questions. It's stilll not clear to me what is being measured here. Here are a few possibilities. First, we might think of these authors (the top 200 across both graph) as centroids in the space. This is by definition, because the values are mean similarities to all other authors. So, what does it mean to be a centroid?

**Number of abstracts:** The more abstracts you write, the more words you use, the more opportunity you have to use the same kinds of words that are used in other abstracts. In other words, authors with more abstracts should tend to have author vectors that have higher mean similarity than authors with fewer abstracts. So are these authors just the ones who have published the most? (note to self to find out).

**Influence vs. Influenced:** It would be interesting if mean similarity reflected something about the influence that authors have on the direction of the field. For example, do authros with high men similarity have more influence on other authors, or have they been more influenced by other authors? For example, perhaps centroid authors were the so-called influencers, they go off in a direction in semantic space and other authors then join in; thereby, surrounding the original author, which causes them to becomd a centroid. On the other hand, something like the opposite seems possible. A centroid author could be your "average" author who is producing derivative works similar to everything else in the field. In other words, you could become a centroid if you were copying a lot of different authors work. This second option seems less likely to explain the data. To my eye, the authors in the above two graphs are all fairly well known and respected authors. So, if the measure is picking up on influence, then what do we have? Still lots of questions. Perhaps a semantically inspired H-index? [^1].

[^1]: More likely, several alternative measures of how an abstract or author is situated within a broader literature. Not that I want to get into the business of advocating for different measures of academic performance. 

**Jack of all trades? Semantic diversity:** Author vectors are the sum of their abstracts. If an author publishes widely across the discipline, that is across semantically diverse areas of the discipline; then, their author vector will be closer to more abstracts in different areas. This would presumably raise their mean similarity to all other authors. Authors who publish only in one area, perhaps one that that is relatively siloed compared to others, should have lower mean similarity to all other authors. So, perhaps this list of top 200 authors partly reflects the semantic diversity of their abstract record [^2].

[^2]: Footnote to self to check this out. It would be easy to deconsruct the author vector into component abstract vectors, and then look at dispersion among the abstract vectors as a measure of semantic diversity. And, I need to update the Shiny app to enable search from an author to their articles. 




---

## References



